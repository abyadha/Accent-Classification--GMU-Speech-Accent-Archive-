{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 775716,
     "status": "ok",
     "timestamp": 1762122228049,
     "user": {
      "displayName": "Ahmed Abyadh",
      "userId": "04994668453682193780"
     },
     "user_tz": 300
    },
    "id": "_jarvP4kDmB6",
    "outputId": "d8f85ea9-ee51-46b0-a651-fb92f13ad067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "======================================================================\n",
      "DOWNLOADING ACCENT DATA: Mandarin, Russian, Spanish\n",
      "======================================================================\n",
      "Output directory: /content/drive/MyDrive/Fall25/EE502/FinalProjects/Ch24/data\n",
      "======================================================================\n",
      "\n",
      "Installing dependencies...\n",
      "✓ Dependencies installed\n",
      "\n",
      "Fetching available languages from GMU website...\n",
      "✓ Found 392 languages on the website\n",
      "\n",
      "======================================================================\n",
      "SEARCHING FOR TARGET ACCENTS\n",
      "======================================================================\n",
      "✓ Found Mandarin: 'mandarin' (157 speakers)\n",
      "✓ Found Russian: 'russian' (82 speakers)\n",
      "✓ Found Spanish: 'spanish' (243 speakers)\n",
      "\n",
      "✓ All 3 accents found! Proceeding with download...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Downloading: MANDARIN\n",
      "======================================================================\n",
      "Speakers to download: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mandarin: 100%|██████████| 157/157 [04:06<00:00,  1.57s/speaker]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Mandarin: 157 files downloaded, 0 failed\n",
      "\n",
      "======================================================================\n",
      "Downloading: RUSSIAN\n",
      "======================================================================\n",
      "Speakers to download: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "russian: 100%|██████████| 82/82 [02:12<00:00,  1.62s/speaker]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Russian: 82 files downloaded, 0 failed\n",
      "\n",
      "======================================================================\n",
      "Downloading: SPANISH\n",
      "======================================================================\n",
      "Speakers to download: 243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spanish: 100%|██████████| 243/243 [06:26<00:00,  1.59s/speaker]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Spanish: 243 files downloaded, 0 failed\n",
      "\n",
      "======================================================================\n",
      "DOWNLOAD COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "✓ Total samples downloaded: 482\n",
      "✓ Combined metadata saved: /content/drive/MyDrive/Fall25/EE502/FinalProjects/Ch24/data/metadata.csv\n",
      "✓ Individual accent folders created in: /content/drive/MyDrive/Fall25/EE502/FinalProjects/Ch24/data\n",
      "\n",
      "======================================================================\n",
      "SUMMARY BY ACCENT\n",
      "======================================================================\n",
      "  Mandarin       :  157 samples\n",
      "  Russian        :   82 samples\n",
      "  Spanish        :  243 samples\n",
      "\n",
      "======================================================================\n",
      "Next Step: Run the SVM benchmark code!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Download Mandarin, Russian, and Spanish Accents from GMU Speech Accent Archive\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# SETUP\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Output directory - UPDATED with Fall25\n",
    "OUTDIR = \"/content/drive/MyDrive/Fall25/EE502/FinalProjects/Ch24/data\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING ACCENT DATA: Mandarin, Russian, Spanish\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Output directory: {OUTDIR}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Install dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q requests beautifulsoup4 tqdm\n",
    "print(\"✓ Dependencies installed\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "import csv\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "BASE = \"https://accent.gmu.edu/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Academic research downloader for Speech Accent Archive \"\n",
    "        \"(Student project - EE502 Final Project)\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def http_get(url: str, *, stream: bool = False, timeout: int = 30) -> requests.Response:\n",
    "    \"\"\"HTTP GET with error handling.\"\"\"\n",
    "    r = requests.get(url, headers=HEADERS, stream=stream, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def polite_sleep(lo: float = 0.6, hi: float = 1.4) -> None:\n",
    "    \"\"\"Sleep to be polite to the server.\"\"\"\n",
    "    time.sleep(random.uniform(lo, hi))\n",
    "\n",
    "def list_all_language_pages() -> Dict[str, str]:\n",
    "    \"\"\"Get all available language/accent pages.\"\"\"\n",
    "    print(\"Fetching available languages from GMU website...\")\n",
    "    url = urljoin(BASE, \"browse_language.php\")\n",
    "    html = http_get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    langs = {}\n",
    "\n",
    "    for a in soup.select(\"a[href*='browse_language.php?'][href*='language=']\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        lang_url = urljoin(BASE, href)\n",
    "        lang_name = (a.text or \"\").strip()\n",
    "\n",
    "        if not lang_name:\n",
    "            q = parse_qs(urlparse(lang_url).query).get(\"language\", [\"\"])[0]\n",
    "            lang_name = q.strip()\n",
    "\n",
    "        if lang_name:\n",
    "            langs[lang_name.lower()] = lang_url\n",
    "\n",
    "    print(f\"✓ Found {len(langs)} languages on the website\\n\")\n",
    "    return langs\n",
    "\n",
    "def extract_speaker_links(lang_url: str, limit: Optional[int] = None) -> List[str]:\n",
    "    \"\"\"Extract speaker detail page links for a language.\"\"\"\n",
    "    html = http_get(lang_url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = [urljoin(BASE, a.get(\"href\")) for a in soup.select(\"a[href*='function=detail']\")]\n",
    "\n",
    "    if limit is not None:\n",
    "        links = links[:limit]\n",
    "\n",
    "    return links\n",
    "\n",
    "def find_first_mp3_url(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"Find the MP3 audio URL on a speaker page.\"\"\"\n",
    "    # Try <a> tags\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.lower().endswith(\".mp3\"):\n",
    "            return urljoin(BASE, href)\n",
    "\n",
    "    # Try <source> tags\n",
    "    src = soup.find(\"source\", src=re.compile(r\"\\.mp3$\", re.I))\n",
    "    if src and src.get(\"src\"):\n",
    "        return urljoin(BASE, src[\"src\"].strip())\n",
    "\n",
    "    return None\n",
    "\n",
    "# Patterns to extract metadata from speaker pages\n",
    "FIELD_PATTERNS = {\n",
    "    \"birth_place\": re.compile(r\"birth place\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "    \"native_language\": re.compile(r\"native language\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "    \"other_languages\": re.compile(r\"other language\\(s\\)\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "    \"age_sex\": re.compile(r\"age,\\s*sex\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "    \"age_english_onset\": re.compile(r\"age of english onset\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "    \"english_learning_method\": re.compile(r\"english learning method\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "    \"years_in_english_country\": re.compile(r\"years in english-speaking country\\s*:\\s*([^\\n\\r]+)\", re.I),\n",
    "}\n",
    "\n",
    "def parse_speaker_page(spk_url: str) -> Dict[str, str]:\n",
    "    \"\"\"Parse speaker page to extract metadata and audio URL.\"\"\"\n",
    "    html = http_get(spk_url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    mp3 = find_first_mp3_url(soup)\n",
    "    text = soup.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    meta = {\"speaker_url\": spk_url, \"audio_url\": mp3 or \"\"}\n",
    "\n",
    "    for key, pat in FIELD_PATTERNS.items():\n",
    "        m = pat.search(text)\n",
    "        meta[key] = m.group(1).strip() if m else \"\"\n",
    "\n",
    "    return meta\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    \"\"\"Convert string to safe filename.\"\"\"\n",
    "    name = re.sub(r\"[^\\w.\\- ]+\", \"_\", name)\n",
    "    return name[:150]\n",
    "\n",
    "def download_file(url: str, dest_path: pathlib.Path) -> None:\n",
    "    \"\"\"Download a file from URL to destination.\"\"\"\n",
    "    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if dest_path.exists() and dest_path.stat().st_size > 0:\n",
    "        return\n",
    "\n",
    "    with http_get(url, stream=True) as r:\n",
    "        with open(dest_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1 << 14):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "def write_csv(csv_path: pathlib.Path, rows: List[Dict[str, str]]) -> None:\n",
    "    \"\"\"Write list of dicts to CSV.\"\"\"\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    headers = sorted({k for row in rows for k in row.keys()})\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=headers)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def count_speakers_for_accent(lang_url: str) -> int:\n",
    "    \"\"\"Count how many speakers are available for an accent.\"\"\"\n",
    "    try:\n",
    "        speakers = extract_speaker_links(lang_url)\n",
    "        return len(speakers)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def download_accent(lang_name: str, lang_url: str, outdir: pathlib.Path,\n",
    "                    limit_per_accent: Optional[int] = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"Download all speakers for one accent.\"\"\"\n",
    "    rows = []\n",
    "    speakers = extract_speaker_links(lang_url, limit=limit_per_accent)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Downloading: {lang_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Speakers to download: {len(speakers)}\")\n",
    "\n",
    "    accent_folder = outdir / safe_filename(lang_name)\n",
    "    accent_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for spk_url in tqdm(speakers, desc=f\"{lang_name}\", unit=\"speaker\"):\n",
    "        try:\n",
    "            meta = parse_speaker_page(spk_url)\n",
    "            audio_url = meta.get(\"audio_url\", \"\")\n",
    "            local_path = \"\"\n",
    "\n",
    "            if audio_url:\n",
    "                fname = safe_filename(os.path.basename(urlparse(audio_url).path) or f\"{hash(spk_url)}.mp3\")\n",
    "                fpath = accent_folder / fname\n",
    "                download_file(audio_url, fpath)\n",
    "                local_path = str(fpath)\n",
    "                success_count += 1\n",
    "                polite_sleep(0.8, 1.6)  # Be polite to server\n",
    "            else:\n",
    "                fail_count += 1\n",
    "\n",
    "            meta.update({\"accent\": lang_name, \"local_audio_path\": local_path})\n",
    "            rows.append(meta)\n",
    "\n",
    "        except Exception as e:\n",
    "            fail_count += 1\n",
    "            rows.append({\n",
    "                \"accent\": lang_name,\n",
    "                \"speaker_url\": spk_url,\n",
    "                \"audio_url\": \"\",\n",
    "                \"local_audio_path\": \"\",\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    # Save individual accent CSV\n",
    "    if rows:\n",
    "        write_csv(accent_folder / f\"{safe_filename(lang_name)}.csv\", rows)\n",
    "\n",
    "    print(f\"\\n✓ {lang_name.capitalize()}: {success_count} files downloaded, {fail_count} failed\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ============================================================\n",
    "# MAIN DOWNLOAD FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def download_three_accents():\n",
    "    \"\"\"Download Mandarin, Russian, and Spanish accents.\"\"\"\n",
    "\n",
    "    outpath = pathlib.Path(OUTDIR)\n",
    "    outpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get all available languages\n",
    "    all_langs = list_all_language_pages()\n",
    "\n",
    "    if not all_langs:\n",
    "        raise RuntimeError(\"Could not fetch languages from website. Check internet connection.\")\n",
    "\n",
    "    # Define the 3 target accents\n",
    "    # Try multiple variations as website might use different names\n",
    "    target_accents = {\n",
    "        'mandarin': ['mandarin', 'chinese mandarin', 'mandarin chinese', 'chinese'],\n",
    "        'russian': ['russian'],\n",
    "        'spanish': ['spanish', 'castilian spanish']\n",
    "    }\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"SEARCHING FOR TARGET ACCENTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Find matching accents on website\n",
    "    found_accents = {}\n",
    "\n",
    "    for accent_key, variations in target_accents.items():\n",
    "        found = False\n",
    "        for variation in variations:\n",
    "            if variation in all_langs:\n",
    "                found_accents[accent_key] = (variation, all_langs[variation])\n",
    "                count = count_speakers_for_accent(all_langs[variation])\n",
    "                print(f\"✓ Found {accent_key.capitalize()}: '{variation}' ({count} speakers)\")\n",
    "                found = True\n",
    "                polite_sleep()\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            print(f\"✗ Could not find {accent_key.capitalize()}\")\n",
    "            print(f\"  Tried: {variations}\")\n",
    "\n",
    "    if len(found_accents) < 3:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ERROR: Could not find all 3 accents!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nAvailable languages on website (first 20):\")\n",
    "        for i, lang in enumerate(sorted(all_langs.keys())[:20], 1):\n",
    "            print(f\"  {i:2d}. {lang}\")\n",
    "        print(\"  ...\")\n",
    "        print(f\"\\nTotal: {len(all_langs)} languages available\")\n",
    "        raise ValueError(f\"Only found {len(found_accents)}/3 target accents\")\n",
    "\n",
    "    print(f\"\\n✓ All 3 accents found! Proceeding with download...\\n\")\n",
    "\n",
    "    # Download each accent\n",
    "    all_rows = []\n",
    "\n",
    "    for accent_key in ['mandarin', 'russian', 'spanish']:\n",
    "        if accent_key in found_accents:\n",
    "            lang_name, lang_url = found_accents[accent_key]\n",
    "            rows = download_accent(lang_name, lang_url, outpath, limit_per_accent=None)\n",
    "            all_rows.extend(rows)\n",
    "        else:\n",
    "            print(f\"\\n⚠ Skipping {accent_key} (not found)\")\n",
    "\n",
    "    # Write combined metadata CSV\n",
    "    if all_rows:\n",
    "        metadata_path = outpath / \"metadata.csv\"\n",
    "        write_csv(metadata_path, all_rows)\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DOWNLOAD COMPLETE!\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\n✓ Total samples downloaded: {len(all_rows)}\")\n",
    "        print(f\"✓ Combined metadata saved: {metadata_path}\")\n",
    "        print(f\"✓ Individual accent folders created in: {outpath}\")\n",
    "\n",
    "        # Show summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SUMMARY BY ACCENT\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        from collections import Counter\n",
    "        accent_counts = Counter(row['accent'] for row in all_rows if 'accent' in row)\n",
    "        for accent, count in sorted(accent_counts.items()):\n",
    "            print(f\"  {accent.capitalize():15}: {count:4d} samples\")\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Next Step: Run the SVM benchmark code!\")\n",
    "        print(f\"{'='*70}\")\n",
    "    else:\n",
    "        print(\"\\n✗ No data downloaded!\")\n",
    "\n",
    "# ============================================================\n",
    "# RUN THE DOWNLOAD\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        download_three_accents()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠ Download interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n✗ Error: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPdzIidZn5hZGT14+cgwS2O",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
